\documentclass[sigconf]{acmart}
 % Do not change for KDD'19

\settopmatter{printacmref=true}
  % mandatory for KDD'19

\fancyhead{}
  % do not delete this code.

\usepackage{balance}
  % for creating a balanced last page (usually last page with references)
  
\usepackage{array}

% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.

\begin{document}

\title{Self-supervised universal embeddings for multivariate time-series}


\begin{abstract}

In this paper we propose to use metric learning approach to encode arbitrary event sequences as fixed-length vectors.

\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010405</concept_id>
<concept_desc>Applied computing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[300]{Applied computing}

\keywords{representation learning, recurrent neural networks, multivariate time-series}

\maketitle

\section{Introduction}

Self-supervised learning has been shown to be extremely effective in many fields, such as natural language processing (ELMO, BERT and other) or computer vision (...). Key advantage of self-supervised learning vs supervised learning, that it doesn't require a tagret, which allows us to use unlabelled or partially labeled datasets. Knowledge, learned through self-supervised learning, can be transferred to a target task via transfer learning (BERT) or embedding extraction (word2vec).

In this paper we present a novel approach for self-supervised encoding of arbitrary event sequences, such as transactions data or clickstream data. Our method is based on metric learning approach. Originally metric learning was proposed for mapping images to a high dimensional feature space. In this space semantically similar images should be close to each other, whereas semantically dissimilar images should be far apart from each other.

We propose the following method of applying metric learning to event sequences: from each sequence, we pick out several subsequences. The network is trained to produce close vectors for subsequences of the same sequence, and distant vectors for subsequences of different sequences. So, in terms of metric learning, classes are sequences, and instances of a class are subsequences.

We consider several open datasets and show that the performance of the model trained only on embeddings extracted via our method is comparable to the performance of the network trained by the supervised method.
Moreover, we show superiority of our embeddings over supervised approach on partially labeled data due to insufficient amount of the target to learn a deep model from scratch.

Furthermore, the network trained on metric learning task can be used as a pre-trained model for a target task. In our experiments we show, that usage the pre-trained model requires significantly less amount of labeled data than supervised approach to achieve the same performance.

The rest of the paper is organised as follows. In Section 2 we discuss the related work. In Section 3 we describe the proposed method. In Section 4 we present our experiments and in Section 5 the results of these experiments. Section 6 and 7 are dedicated to the discussion of our results and conclusions.

\section{Related work} \label{sec-rw}

Embeddings have long history of successful usage in NLP applications to represent documents as a fixed-length vectors. There are some efforts to generalize embeddings approaches for more broad cases \cite{Wu2017StarSpaceEA}.

Metric learning approach for imaging was proposed in \cite{Hadsell:2006:DRL:1153171.1153654}.
Deep Metric learning is the often used for face recognition \cite{Schroff2015FaceNetAU}, \cite{kaya2019deep}.

In the recent publication \cite{reimers-2019-sentence-bert} BERT-based model \cite{Devlin2019BERTPO} trained using metric learning loss is used for different NPL tasks.

Metric learning has also a rich story of usage in recommender systems \cite{Hsieh:2017:CML:3038912.3052639}.

There are several examples of using a single set of embeddings for several downstream tasks \cite{Song2017LearningUE}, \cite{Zhai:2019:LUE:3292500.3330739}.

\section{The method}

\subsection{Event series data}

\subsection{Metric learning}

\subsection{Encoder architecture}

There can be several approaches which can be used to encode a sequence of events as a fixed-size vector. One possible approach is to use the recurrent network as in \cite{Sutskever:2014:SSL:2969033.2969173}. The other approach is to use the encoder part of the Transformer architecture presented in \cite{DBLP:journals/corr/VaswaniSPUJGKP17}. In both cases the output produced for the last event can be used to represent the whole series of events.

\section{Experiments} \label{sec-exp}


\subsection{Datasets} \label{sec-datasets}
\begin{enumerate}
    \item \textbf{DataLike}\footnote{https://vc.ru/data-like}, a story like prediction competition.
    \item \textbf{Age prediction competition}\footnote{https://onti.ai-academy.ru/competition}, a competition of using bank transactions for age prediction.
    \item \textbf{Gender prediction competition}\footnote{https://www.kaggle.com/c/python-and-analyze-data-final-project/}, a competition of using bank transactions for gender prediction.
\end{enumerate}

\section{Results}

\section{Conclusions}

\bibliographystyle{ACM-Reference-Format}
\bibliography{sigconf}

\end{document}
